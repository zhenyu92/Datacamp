{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop clop clop] SOLDIER #1: Halt!  Who goes there? ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'son', 'sovereign', 'England', 'clop', '[', 'KING', 'is', 'It', 'SOLDIER', 'I', 'King', 'defeator', 'of', 'there', 'ARTHUR', 'the', 'Pendragon', 'wind', ']', '#', 'Arthur', 'SCENE', 'Camelot', '!', 'all', '.', 'Whoa', 'castle', 'Who', '?', 'Uther', '1', 'Saxons', ',', ':', 'goes', 'from', 'Britons', 'Halt'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 21\n",
      "<_sre.SRE_Match object; span=(9, 75), match='[wind] [clop clop clop] KING ARTHUR: Whoa there! >\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"clop\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = \"Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿      [wind]\n",
      "      [clop clop]\n",
      "  ARTHUR:  Whoa there!\n",
      "      [clop clop]\n",
      " \n",
      "  GUARD #1:  Halt!  Who goes there?\n",
      "  ARTHUR:  It is I, Arthur, son of Uther Pendragon, from the castle\n",
      "      of Camelot.  King of the Britons, defeator of the Saxons, sovereign\n",
      "      of all England!\n",
      "  GUARD #1:  Pull the other one!\n",
      "  ARTHUR:  I am.  And this my trusty servant Patsy.\n",
      "      We have ridden the length and breadth of the land in search of knights\n",
      "      who will join me in my court of Camelot.  I must speak with your lord\n",
      "      and master.\n",
      "  GUARD #1:  What, ridden on a horse?\n",
      "  ARTHUR:  Yes!\n",
      "  GUARD #1:  You're using coconuts!\n",
      "  ARTHUR:  What?\n",
      "  GUARD #1:  You've got two empty halves of coconut and you're bangin'\n",
      "      'em together.\n",
      "  ARTHUR:  So?  We have ridden since the snows of winter covered this\n",
      "      land, through the kingdom of Mercea, through--\n",
      "  GUARD #1:  Where'd you get the coconut?\n",
      "  ARTHUR:  We found them.\n",
      "  GUARD #1:  Found them?  In Mercea?  The coconut's tropical!\n",
      "  ARTHUR:  What do you mean?\n",
      "  GUARD #1:  Well, this is a temperate zone.\n",
      "  ARTHUR:  The swallow may fly south with the sun or the house martin\n",
      "      or the plumber may seek warmer climes in winter yet these are not\n",
      "      strangers to our land.\n",
      "  GUARD #1:  Are you suggesting coconuts migrate?\n",
      "  ARTHUR:  Not at all, they could be carried.\n",
      "  GUARD #1:  What -- a swallow carrying a coconut?\n",
      "  ARTHUR:  It could grip it by the husk!\n",
      "  GUARD #1:  It's not a question of where he grips it!  It's a simple\n",
      "      question of weight ratios!  A five ounce bird could not carry a 1 pound\n",
      "      coconut.\n",
      "  ARTHUR:  Well, it doesn't matter.  Will you go and tell your master\n",
      "      that Arthur from the Court of Camelot is here.\n",
      "  GUARD #1:  Listen, in order to maintain air-speed velocity, a swallow\n",
      "      needs to beat its wings 43 times every second, right?\n",
      "  ARTHUR:  Please!\n",
      "  GUARD #1:  Am I right?\n",
      "  ARTHUR:  I'm not interested!\n",
      "  GUARD #2:  It could be carried by an African swallow!\n",
      "  GUARD #1:  Oh, yeah, an African swallow maybe, but not a European\n",
      "      swallow, that's my point.\n",
      "  GUARD #2:  Oh, yeah, I agree with that...\n",
      "  ARTHUR:  Will you ask your master if he wants to join my court\n",
      "      at Camelot?!\n",
      "  GUARD #1:  But then of course African swallows are not migratory.\n",
      "  GUARD #2:  Oh, yeah...\n",
      "  GUARD #1:  So they couldn't bring a coconut back anyway...\n",
      "      [clop clop]\n",
      "  GUARD #2:  Wait a minute -- supposing two swallows carried it together?\n",
      "  GUARD #1:  No, they'd have to have it on a line.\n",
      "  GUARD #2:  Well, simple!  They'd just use a standard creeper!\n",
      "  GUARD #1:  What, held under the dorsal guiding feathers?\n",
      "  GUARD #2:  Well, why not?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('scene_one.txt', 'r') as f:\n",
    "    scene_one = f.read()\n",
    "print(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple topic identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Wikipedia articles/wiki_text_debugging.txt', 'r') as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english_stopwords.txt', 'r') as f:\n",
    "    english_stops = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "txt_files = glob.glob(\"Wikipedia articles/*.txt\")\n",
    "articles = []\n",
    "for txt_file in txt_files:\n",
    "    with open('{}'.format(txt_file), 'r') as f:\n",
    "        article = f.read()\n",
    "        tokens = word_tokenize(article)\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "        no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "        articles.append(lemmatized)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(4, 1), (6, 6), (7, 2), (9, 5), (18, 1), (19, 1), (20, 1), (22, 11), (23, 1), (25, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 351\n",
      "program 79\n",
      "machine 76\n",
      "first 61\n",
      "cite 59\n",
      "computer 753\n",
      "software 450\n",
      "program 341\n",
      "cite 322\n",
      "language 320\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools \n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.005192624003113299), (6, 0.005170489909089006), (7, 0.00827585860171002), (9, 0.025963120015566495), (18, 0.0032761809826735086)]\n",
      "transistor 0.18791977549536507\n",
      "mechanical 0.18631370968730976\n",
      "circuit 0.17036141109902245\n",
      "manchester 0.1439696847583757\n",
      "alu 0.14093983162152382\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id),weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News articles/uber_apple.txt', 'r') as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary = True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x1a281da620>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary = True)\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = collections.defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "print(chunked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing NLTK with spaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG \n",
      "\n",
      "\n",
      "Dans\n",
      "ORG Monde\n",
      "PERSON Charles Cuvelliez\n",
      "PERSON de lâ€™homme\n",
      "NORP lâ€™algorithme\n",
      "CARDINAL 10.05.2017\n",
      "CARDINAL 10.05.2017\n",
      "CARDINAL 09h47\n",
      "PERSON Par Charles Cuvelliez\n",
      "ORG Professeur\n",
      "GPE de l'universitÃ© libre de Bruxelles\n",
      "ORG \n",
      "\n",
      "\n",
      "TRIBUNE\n",
      "ORG de Facebook Live\n",
      "FAC de 3\n",
      "ORG lâ€™intelligence\n",
      "GPE quâ€™on\n",
      "PERSON \n",
      "\n",
      "\n",
      "\n",
      "NORP nâ€™a\n",
      "DATE 1955\n",
      "DATE quâ€™on\n",
      "NORP quâ€™il\n",
      "DATE dâ€™une\n",
      "NORP \n",
      "\n",
      "\n",
      "Comme le\n",
      "WORK_OF_ART Livre\n",
      "ORG Pourquoi\n",
      "PRODUCT de lâ€™Intelligence\n",
      "PERSON Julien Maldonato\n",
      "ORG Deloitte\n",
      "DATE 2017\n",
      "PERSON \n",
      "\n",
      "\n",
      "\n",
      "PERSON Il sâ€™agit\n",
      "NORP nâ€™explique\n",
      "PERSON \n",
      "\n",
      "\n",
      "\n",
      "CARDINAL 29\n",
      "DATE 2017\n",
      "ORG lâ€™IA\n"
     ]
    }
   ],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French NER with polyglot I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News articles/french.txt', 'r') as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French NER with polyglot II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News articles/spanish.txt', 'r') as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "    \n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'MÃ¡rquez' or 'Gabo'\n",
    "    if \"MÃ¡rquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillaryâ€™s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>â€” Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "8476                        You Can Smell Hillaryâ€™s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142  â€” Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875    It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv', index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size = 0.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df =  0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   Ø­Ù„Ø¨  Ø¹Ø±Ø¨ÙŠ  Ø¹Ù†  Ù„Ù…  Ù…Ø§  Ù…Ø­Ø§ÙˆÙ„Ø§Øª  Ù…Ù†  Ù‡Ø°Ø§  ÙˆØ§Ù„Ù…Ø±Ø¶Ù‰  à¸¢à¸‡ade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   Ø­Ù„Ø¨  Ø¹Ø±Ø¨ÙŠ   Ø¹Ù†   Ù„Ù…   Ù…Ø§  Ù…Ø­Ø§ÙˆÙ„Ø§Øª   Ù…Ù†  Ù‡Ø°Ø§  ÙˆØ§Ù„Ù…Ø±Ø¶Ù‰  à¸¢à¸‡ade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(tfidf_df.columns) - set(count_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiren/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha = alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728882, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.4111484102034755, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.72477133248804, 'clinton'), (-6.565395438992684, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
